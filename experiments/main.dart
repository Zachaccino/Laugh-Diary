import 'dart:async';
import 'dart:typed_data';

import 'package:flutter/material.dart';
import 'package:tflite/tflite.dart';
import 'package:flutter_audio_capture/flutter_audio_capture.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({Key? key}) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const MyHomePage(title: 'Flutter Demo Home Page'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({Key? key, required this.title}) : super(key: key);
  final String title;

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {

  // microphone
  FlutterAudioCapture recorder = FlutterAudioCapture();
  bool listening = false;

  int _counter = 0;

  void _incrementCounter() {
    setState(() {
      _counter++;
    });
  }

  /// Audio buffer on filled callback
  /// when microphone recorded enough data on the buffer, this function is called
  /// and we can potentially use it to push buffer to mfcc featrue stream.
  void listener(dynamic obj) {
    var buffer = Float64List.fromList(obj.cast<double>());
    print(buffer);
  }

  void onError(Object e) {
    print(e);
  }

  /// MFCC Config Description:
  /// sampleRate - signal sampling rate
  /// 44,100
  ///
  /// fftSize - number of fft generated by window
  /// 2048
  ///
  /// numFilters - number of MEL filters
  /// 128
  ///
  /// numCoefs - number of cepstral coefficient to keep
  /// 20
  ///
  /// energy - if true, replaces the first value by the window log-energy
  /// false
  ///
  /// preEmphasis - apply signal preEmphasis
  /// 0
  ///
  /// windowLength - window length in number of samples
  /// SAME AS FTTSIZE



  /// Generate a dummy image for our CNN model.
  /// Note that MFCC values are between 0 - 1, whereas an tflite image is 0 - 255.
  /// the model needs to be trained with MFCC normalised to 0 - 255, and MFCC
  /// output should be scaled to 0-255.
  /// DATATYPE: https://api.dart.dev/stable/2.13.4/dart-typed_data/Uint8List-class.html
  /// DATATYPE: https://api.flutter.dev/flutter/dart-typed_data/Float32List-class.html
  Uint8List dummyMFCC() {
    // MFCC has 20 coefficient bands.
    var bands = 20;
    // a single classification is made up of 20 frames.
    var frames = 20;
    // output of the MFCC library is List<double>.
    var rawMFCC = List.filled(bands*frames, 0.0001);
    // our model uses float32 flattened input.
    var rawInput = Float32List.fromList(rawMFCC);
    // tflite only takes uint8, segment float32 into uint8 chunks.
    return rawInput.buffer.asUint8List();
  }

  /// a tensorflow laugh detection demo.
  /// This is an async method.
  /// For documentation, you need to read the source code.
  /// EXAMPLE: https://github.com/shaqian/flutter_tflite/blob/master/example/lib/main.dart
  /// TESTS: https://github.com/shaqian/flutter_tflite/blob/master/test/tflite_test.dart
  /// SOURCE: https://github.com/shaqian/flutter_tflite/blob/master/lib/tflite.dart
  /// Architecture: MIC INPUT STREAM -> MFCC FEATURE STREAM -> CUMULATE -> PREDICT
  ///
  /// Essentially, we need to stream data from mic input into MFCC feature extractor.
  /// MFCC feature extractor has an output stream that produces MFCCs. The streaming
  /// data are discrete frames. We need to make sure the Librosa and Flutter MFCC
  /// has the same Fast Fourier Transform setting, such that the framing and sampling
  /// is identical. A frame can generate one MFCC band, and we need to cumulate
  /// 20 MFCC bands for each prediction. This determines the temporal resolution
  /// of our detection method. Anyway, once we have 20 bands, we can predict
  /// the result. 406 frames should be around 0.5 seconds, we can do a majority
  /// vote to aggregate the prediction so its less susceptible to noisy environment.
  ///
  /// Helpful Tools:
  /// Netron Model Visualiser: https://github.com/lutzroeder/netron
  tfDemo() async {
    // load model
    await Tflite.loadModel(
        model: "assets/laugh.tflite", labels: "assets/labels.txt");

    // load mfcc
    var mfcc = dummyMFCC();

    // predict laughing or talking
    var y = await Tflite.runModelOnBinary(binary: mfcc);

    // output result to console for now
    print(y);

    if (listening) {
      print("Cancel Listening");
      await recorder.stop();
      listening = false;
    } else {
      print("Start Listening");
      // Note that you need to go to emulator, long press the app, select app info
      // go to Microphone permission, and enable it manually. Im too lazy to ask
      // for permission...
      await recorder.start(listener, onError, sampleRate: 16000, bufferSize: 3000);
      listening = true;
    }
  }


  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text(widget.title),
      ),
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: <Widget>[
            const Text(
              'You have clicked the button this many times:',
            ),
            Text(
              '$_counter',
              style: Theme.of(context).textTheme.headline4,
            ),
          ],
        ),
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: tfDemo,
        tooltip: 'Increment',
        child: const Icon(Icons.add),
      ), // This trailing comma makes auto-formatting nicer for build methods.
    );
  }
}
