import 'dart:async';
import 'dart:typed_data';

import 'package:flutter/material.dart';
import 'package:tflite/tflite.dart';
// import 'package:flutter_sound/flutter_sound.dart';
import 'package:logger/logger.dart';
import 'package:flutter_audio_capture/flutter_audio_capture.dart';
import 'package:dio/dio.dart';

/// add "--no-sound-null-safety" in `Edit Configuration -> Additiona run args`
/// This package it not null safety rated.
import 'package:mfcc/mfcc.dart';

/// shader compilation error
/// add "--enable-software-rendering" in `Edit Configuration -> Additiona run args`



void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({Key? key}) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const MyHomePage(title: 'Flutter Demo Home Page'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({Key? key, required this.title}) : super(key: key);
  final String title;

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {

  // audio capturing
  var listening = false;
  // var recorder = FlutterSoundRecorder(logLevel: Level.error);
  // StreamController<Food>? audioInput;
  // StreamSubscription<Food>? streamSub;
  var plugin = FlutterAudioCapture();
  List<double> frame = List.from([]);
  var dio = Dio();
  var sending = false;

  int _counter = 0;

  void _incrementCounter() {
    setState(() {
      _counter++;
    });
  }


  /// record sound from microphone.
  /// Note that you need to go to emulator, long press the app, select app info
  /// go to Microphone permission, and enable it manually. Im too lazy to ask
  /// for permission...
  startRecording() async {
    // await recorder.openAudioSession();
    // audioInput = StreamController<Food>();
    // streamSub = audioInput!.stream.listen((buffer) async{
    //   if (buffer is FoodData) {
    //     frame.addAll(buffer.data!.buffer.asFloat32List());
    //     if (frame.length >= 12000) {
    //       List<double> data = frame.take(12000).toList();
    //       print(data);
    //       // print(frame.length);
    //       // print(data.length);
    //
    //       var features = MFCC.mfccFeats(data, 22050, 2048, 512, 2048, 128, 20);
    //       var flattened = features.expand((i) => i).toList();
    //       // print(features[19]);
    //       var input = Float32List.fromList(flattened).buffer.asUint8List();
    //       var y = await Tflite.runModelOnBinary(binary: input);
    //       // output result to console for now
    //       print(y);
    //       print("Nani");
    //       frame = [];
    //     }
    //   }
    // });
    // await recorder.startRecorder(
    //   toStream: audioInput!.sink,
    //   codec: Codec.pcm16,
    //   numChannels: 1,
    //   sampleRate: 22050,
    // );

  }

  listener(dynamic obj) async {
    var buffer = Float64List.fromList(obj.cast<double>()).toList();
    frame.addAll(buffer);
    var targetLen = 15000;
    if (frame.length >= targetLen && !sending) {
      sending = true;
      List<double> data = frame.take(targetLen).toList();
      var features = MFCC.mfccFeats(data, 22050, 1024, 512, 512, 20, 20);
      var response = await dio.post('http://172.16.0.5:5000/', data: {'mfcc': features}).catchError(onError);
      print(response);


      // var flattened = features.expand((i) => i).toList();
      // var input = Float32List.fromList(flattened).buffer.asUint8List();
      // var y = await Tflite.runModelOnBinary(binary: input);
      // print(y);
      // print("Nani");

      frame = [];
      sending = false;
    }

  }
  void onError(Object e) {
    print(e);
  }


  stopRecording() async {
    // await recorder.stopRecorder();
    // streamSub!.cancel();
    // await recorder.closeAudioSession();
    // Should close the recorder as well when done.
    // Not included here since I don't have extra buttons.
  }


  /// MFCC Config Description:
  /// sampleRate - signal sampling rate
  /// 22500
  ///
  /// fftSize - number of fft generated by window
  /// 2048
  ///
  /// numFilters - number of MEL filters
  /// 128
  ///
  /// numCoefs - number of cepstral coefficient to keep
  /// 20
  ///
  /// energy - if true, replaces the first value by the window log-energy
  /// false
  ///
  /// preEmphasis - apply signal preEmphasis
  /// 0
  ///
  /// windowLength - window length in number of samples
  /// SAME AS FTTSIZE
  ///
  /// windowStride
  /// 512



  /// Generate a dummy image for our CNN model.
  /// Note that MFCC values are between 0 - 1, whereas an tflite image is 0 - 255.
  /// the model needs to be trained with MFCC normalised to 0 - 255, and MFCC
  /// output should be scaled to 0-255.
  /// DATATYPE: https://api.dart.dev/stable/2.13.4/dart-typed_data/Uint8List-class.html
  /// DATATYPE: https://api.flutter.dev/flutter/dart-typed_data/Float32List-class.html
  Uint8List dummyMFCC() {
    // MFCC has 20 coefficient bands.
    var bands = 20;
    // a single classification is made up of 20 frames.
    var frames = 20;
    // output of the MFCC library is List<double>.
    var rawMFCC = List.filled(bands*frames, 0.0001);
    // our model uses float32 flattened input.
    var rawInput = Float32List.fromList(rawMFCC);
    // tflite only takes uint8, segment float32 into uint8 chunks.
    return rawInput.buffer.asUint8List();
  }

  /// a tensorflow laugh detection demo.
  /// This is an async method.
  /// For documentation, you need to read the source code.
  /// EXAMPLE: https://github.com/shaqian/flutter_tflite/blob/master/example/lib/main.dart
  /// TESTS: https://github.com/shaqian/flutter_tflite/blob/master/test/tflite_test.dart
  /// SOURCE: https://github.com/shaqian/flutter_tflite/blob/master/lib/tflite.dart
  /// Architecture: MIC INPUT STREAM -> MFCC FEATURE STREAM -> CUMULATE -> PREDICT
  ///
  /// Essentially, we need to stream data from mic input into MFCC feature extractor.
  /// MFCC feature extractor has an output stream that produces MFCCs. The streaming
  /// data are discrete frames. We need to make sure the Librosa and Flutter MFCC
  /// has the same Fast Fourier Transform setting, such that the framing and sampling
  /// is identical. A frame can generate one MFCC band, and we need to cumulate
  /// 20 MFCC bands for each prediction. This determines the temporal resolution
  /// of our detection method. Anyway, once we have 20 bands, we can predict
  /// the result. 406 frames should be around 0.5 seconds, we can do a majority
  /// vote to aggregate the prediction so its less susceptible to noisy environment.
  ///
  /// Helpful Tools:
  /// Netron Model Visualiser: https://github.com/lutzroeder/netron
  tfDemo() async {
    // load model
    await Tflite.loadModel(
        model: "assets/laugh.tflite", labels: "assets/labels.txt");

    // load mfcc
    // var mfcc = dummyMFCC();

    // predict laughing or talking
    // var y = await Tflite.runModelOnBinary(binary: mfcc);

    // output result to console for now
    // print(y);

    if (listening) {
      print("Cancel Listening");
      await plugin.stop();
      listening = false;
    } else {
      print("Start Listening");
      await plugin.start(listener, onError, sampleRate: 22050, bufferSize: 3000);
      listening = true;
    }
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text(widget.title),
      ),
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: <Widget>[
            const Text(
              'You have clicked the button this many times:',
            ),
            Text(
              '$_counter',
              style: Theme.of(context).textTheme.headline4,
            ),
          ],
        ),
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: tfDemo,
        tooltip: 'Increment',
        child: const Icon(Icons.add),
      ), // This trailing comma makes auto-formatting nicer for build methods.
    );
  }
}
